{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# setting a seed for result reproduceability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "\n",
    "text = \"def greet(user): print(f'hello <extra_id_0>!')\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: {user.username} {user.password} {user.email} {user.password} hello\\n\\nHello,\n",
      "1: world! {user.name} {user.email} {user.password} {user.password}\n",
      "2: {user.username} @ {user.email} hello{user.email}\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=50, \n",
    "        top_p=0.95, # dummy example: if the next word probs are \"I\": 75%, \"You\": 10%, \"We\": \"5%\", etc. etc. with smaller %ages, a value of 0.9 would make the model sample from just these 3 and ignore the rest\n",
    "        top_k=50, # filter sampling pool to n words\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3 # number of samples to generate\n",
    "    )\n",
    "\n",
    "utils.print_generated_sample_outputs(sample_outputs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('savedmodels/codet5_condode_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: void function ( String arg0 ) { System. out. println ( \"srini_string\" + arg0 ) ; }\n",
      "1: void function ( String arg0 ) { System. out. println ( arg0. replaceAll ( \"srini_string\", \"srini_string\" ) ) ; System. out. println ( \"srini_string\" )\n",
      "2: void function ( String arg0 ) { System. out. print ( \"srini_string\" ) ; System. out. println ( arg0 ) ; }\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=50, \n",
    "        top_p=0.95, # dummy example: if the next word probs are \"I\": 75%, \"You\": 10%, \"We\": \"5%\", etc. etc. with smaller %ages, a value of 0.9 would make the model sample from just these 3 and ignore the rest\n",
    "        top_k=50, # filter sampling pool to n words\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3 # number of samples to generate\n",
    "    )\n",
    "\n",
    "utils.print_generated_sample_outputs(sample_outputs, tokenizer)\n",
    "# lmao this is worse, TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: ArrayList function ( int arg0, int arg1, HashMap arg2 ) { ArrayList loc0 = new ArrayList ( ) ; if ( arg0 > 0 ) { for ( int loc1 = 0 ; loc1 < arg0 ; loc\n",
      "1: ArrayList function ( ) { ArrayList loc0 = new ArrayList ( ) ; if ( arg0!= null ) { for ( int loc1 = 0 ; loc1 < functionNum ; loc1 ++ ) { loc0. add ( arg0\n",
      "2: ArrayList function ( ) { ArrayList loc0 = new ArrayList ( ) ; for ( int loc1 = 0 ; loc1 < functionNum ; loc1 ++ ) { loc0. add ( arg0. getFunctionName ( loc1 ) )\n"
     ]
    }
   ],
   "source": [
    "text = \"{\\\"code\\\": \\\"void function ( ScriptOrFnNode arg0 ) { int loc0 = - 1 ; collectFuncNodes ( arg0 , loc0 , null ) ; }\\\", \\\"nl\\\": \\\"generate mappings for each function node and parameters and variables names associated with it . concode_field_sep int parentScope concode_elem_sep ArrayList functionBracePositions concode_elem_sep ObjArray funcObjects concode_elem_sep int functionNum concode_elem_sep ArrayList functionVarMappings concode_elem_sep int lastTokenCount concode_elem_sep ArrayList replacedTokens concode_field_sep boolean isInScopeChain concode_elem_sep void reset concode_elem_sep void leaveNestingLevel concode_elem_sep String getMappedToken concode_elem_sep String getPreviousTokenMapping concode_elem_sep void collectFuncNodes concode_elem_sep int sourceCompress concode_elem_sep void enterNestingLevel\\\"}\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "sample_outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=50, \n",
    "        top_p=0.95, # dummy example: if the next word probs are \"I\": 75%, \"You\": 10%, \"We\": \"5%\", etc. etc. with smaller %ages, a value of 0.9 would make the model sample from just these 3 and ignore the rest\n",
    "        top_k=50, # filter sampling pool to n words\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3 # number of samples to generate\n",
    "    )\n",
    "\n",
    "utils.print_generated_sample_outputs(sample_outputs, tokenizer)\n",
    "# lmao this is worse, TODO "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76293e17429ae2c839469a84a4692f69b1d764ad81a6a044a99c52430bb10e84"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
