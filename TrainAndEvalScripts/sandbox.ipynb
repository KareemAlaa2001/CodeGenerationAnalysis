{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# setting a seed for result reproduceability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "\n",
    "text = \"def greet(user): print(f'hello <extra_id_0>!')\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: {user.username} {user.password} {user.email} {user.password} hello\\n\\nHello,\n",
      "1: world! {user.name} {user.email} {user.password} {user.password}\n",
      "2: {user.username} @ {user.email} hello{user.email}\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=50, \n",
    "        top_p=0.95, # dummy example: if the next word probs are \"I\": 75%, \"You\": 10%, \"We\": \"5%\", etc. etc. with smaller %ages, a value of 0.9 would make the model sample from just these 3 and ignore the rest\n",
    "        top_k=50, # filter sampling pool to n words\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3 # number of samples to generate\n",
    "    )\n",
    "\n",
    "utils.print_generated_sample_outputs(sample_outputs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('savedmodels/codet5_condode_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: void function ( String arg0 ) { System. out. println ( \"srini_string\" + arg0 ) ; }\n",
      "1: void function ( String arg0 ) { System. out. println ( arg0. replaceAll ( \"srini_string\", \"srini_string\" ) ) ; System. out. println ( \"srini_string\" )\n",
      "2: void function ( String arg0 ) { System. out. print ( \"srini_string\" ) ; System. out. println ( arg0 ) ; }\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=50, \n",
    "        top_p=0.95, # dummy example: if the next word probs are \"I\": 75%, \"You\": 10%, \"We\": \"5%\", etc. etc. with smaller %ages, a value of 0.9 would make the model sample from just these 3 and ignore the rest\n",
    "        top_k=50, # filter sampling pool to n words\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3 # number of samples to generate\n",
    "    )\n",
    "\n",
    "utils.print_generated_sample_outputs(sample_outputs, tokenizer)\n",
    "# lmao this is worse, TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: ArrayList function ( int arg0, int arg1, HashMap arg2 ) { ArrayList loc0 = new ArrayList ( ) ; if ( arg0 > 0 ) { for ( int loc1 = 0 ; loc1 < arg0 ; loc\n",
      "1: ArrayList function ( ) { ArrayList loc0 = new ArrayList ( ) ; if ( arg0!= null ) { for ( int loc1 = 0 ; loc1 < functionNum ; loc1 ++ ) { loc0. add ( arg0\n",
      "2: ArrayList function ( ) { ArrayList loc0 = new ArrayList ( ) ; for ( int loc1 = 0 ; loc1 < functionNum ; loc1 ++ ) { loc0. add ( arg0. getFunctionName ( loc1 ) )\n"
     ]
    }
   ],
   "source": [
    "text = \"{\\\"code\\\": \\\"void function ( ScriptOrFnNode arg0 ) { int loc0 = - 1 ; collectFuncNodes ( arg0 , loc0 , null ) ; }\\\", \\\"nl\\\": \\\"generate mappings for each function node and parameters and variables names associated with it . concode_field_sep int parentScope concode_elem_sep ArrayList functionBracePositions concode_elem_sep ObjArray funcObjects concode_elem_sep int functionNum concode_elem_sep ArrayList functionVarMappings concode_elem_sep int lastTokenCount concode_elem_sep ArrayList replacedTokens concode_field_sep boolean isInScopeChain concode_elem_sep void reset concode_elem_sep void leaveNestingLevel concode_elem_sep String getMappedToken concode_elem_sep String getPreviousTokenMapping concode_elem_sep void collectFuncNodes concode_elem_sep int sourceCompress concode_elem_sep void enterNestingLevel\\\"}\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "sample_outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=50, \n",
    "        top_p=0.95, # dummy example: if the next word probs are \"I\": 75%, \"You\": 10%, \"We\": \"5%\", etc. etc. with smaller %ages, a value of 0.9 would make the model sample from just these 3 and ignore the rest\n",
    "        top_k=50, # filter sampling pool to n words\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3 # number of samples to generate\n",
    "    )\n",
    "\n",
    "utils.print_generated_sample_outputs(sample_outputs, tokenizer)\n",
    "# lmao this is worse, TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration, T5Config\n",
    "import torch\n",
    "\n",
    "# setting a seed for result reproduceability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "config = T5Config.from_pretrained('Salesforce/codet5-small')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('./CodeT5/CodeT5/sh/saved_models/codecontest/codet5_small_100000_lr10_bs2_src320_trg150_pat3_e30/checkpoint-last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"RATING: 1900 \\n TAGS: dp \\n LANGUAGE IS cpp \\n CORRECT SOLUTION \\n Ivan unexpectedly saw a present from one of his previous birthdays. It is array of n numbers from 1 to 200. Array is old and some numbers are hard to read. Ivan remembers that for all elements at least one of its neighbours ls not less than it, more formally:\\n\\na_{1} \\u2264 a_{2},\\n\\na_{n} \\u2264 a_{n-1} and\\n\\na_{i} \\u2264 max(a_{i-1},    a_{i+1}) for all i from 2 to n-1.\\n\\nIvan does not remember the array and asks to find the number of ways to restore it. Restored elements also should be integers from 1 to 200. Since the number of ways can be big, print it modulo 998244353.\\n\\nInput\\n\\nFirst line of input contains one integer n (2 \\u2264 n \\u2264 10^{5}) \\u2014 size of the array.\\n\\nSecond line of input contains n integers a_{i} \\u2014 elements of array. Either a_{i} = -1 or 1 \\u2264 a_{i} \\u2264 200. a_{i} = -1 means that i-th element can't be read.\\n\\nOutput\\n\\nPrint number of ways to restore the array modulo 998244353.\\n\\nExamples\\n\\nInput\\n\\n3\\n1 -1 2\\n\\n\\nOutput\\n\\n1\\n\\n\\nInput\\n\\n2\\n-1 -1\\n\\n\\nOutput\\n\\n200\\n\\nNote\\n\\nIn the first example, only possible value of a_{2} is 2.\\n\\nIn the second example, a_{1} = a_{2} so there are 200 different values because all restored elements should be integers between 1 and 200.\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: n = gets.chomp.to_i\n",
      "xl = Hash.new\n",
      "n.times do\n",
      "  x,l = gets.chomp.split.map(&:to_i)\n",
      "  xl[x] = l\n",
      "end\n",
      "\n",
      "memo = [0,0]\n",
      "Hash[xl.sort].each do |k, v|\n",
      "  xl.delete(k) if memo.inject(:+) >= k.to_i\n",
      "  memo = [k.to_i, v]\n",
      "end\n",
      "\n",
      "p xl.count\n",
      "1: n = gets.chomp.to_i\n",
      "xl = Hash.new\n",
      "n.times do\n",
      "  x,l = gets.chomp.split.map(&:to_i)\n",
      "  xl[x] = l\n",
      "end\n",
      "\n",
      "memo = [0,0]\n",
      "Hash[xl.sort].each do |k, v|\n",
      "  xl.delete(k) if memo.inject(:+) >= k.to_i\n",
      "  memo = [k.to_i, v]\n",
      "end\n",
      "\n",
      "p xl.count\n",
      "2: n = gets.chomp.to_i\n",
      "xl = Hash.new\n",
      "n.times do\n",
      "  x,l = gets.chomp.split.map(&:to_i)\n",
      "  xl[x] = l\n",
      "end\n",
      "\n",
      "memo = [0,0]\n",
      "Hash[xl.sort].each do |k, v|\n",
      "  xl.delete(k) if memo.inject(:+) >= k.to_i\n",
      "  memo = [k.to_i, v]\n",
      "end\n",
      "\n",
      "p xl.count\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=256, \n",
    "        top_p=0.95, # dummy example: if the next word probs are \"I\": 75%, \"You\": 10%, \"We\": \"5%\", etc. etc. with smaller %ages, a value of 0.9 would make the model sample from just these 3 and ignore the rest\n",
    "        top_k=50, # filter sampling pool to n words\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3 # number of samples to generate\n",
    "    )\n",
    "\n",
    "utils.print_generated_sample_outputs(sample_outputs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76293e17429ae2c839469a84a4692f69b1d764ad81a6a044a99c52430bb10e84"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
