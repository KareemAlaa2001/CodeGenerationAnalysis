{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# setting a seed for result reproduceability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "\n",
    "text = \"def greet(user): print(f'hello <extra_id_0>!')\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: {user.username}, hello {user.password}, hello {user.name},\n",
      "1: {user} ({user.email}@{user.name}).whoami.com, f'Hello {user.email}: {user.email}@{user.name}.whoami.com\n",
      "2: {user.username} world: {user.nickname} {user.email} world: {user.email}\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=50, \n",
    "        top_p=0.95, # dummy example: if the next word probs are \"I\": 75%, \"You\": 10%, \"We\": \"5%\", etc. etc. with smaller %ages, a value of 0.9 would make the model sample from just these 3 and ignore the rest\n",
    "        top_k=50, # filter sampling pool to n words\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3 # number of samples to generate\n",
    "    )\n",
    "\n",
    "utils.print_generated_sample_outputs(sample_outputs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('savedmodels/codet5_condode_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: void function ( String arg0 ) { System. out. println ( \"srini_string\" + arg0 ) ; }\n",
      "1: void function ( String arg0 ) { System. out. println ( \"srini_string\" + arg0 + \"srini_string\" ) ; }\n",
      "2: void function ( String arg0 ) { System. out. println ( arg0 + \"srini_string\" ) ; System. out. println ( \"srini_string\" ) ; }\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=50, \n",
    "        top_p=0.95, # dummy example: if the next word probs are \"I\": 75%, \"You\": 10%, \"We\": \"5%\", etc. etc. with smaller %ages, a value of 0.9 would make the model sample from just these 3 and ignore the rest\n",
    "        top_k=50, # filter sampling pool to n words\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3 # number of samples to generate\n",
    "    )\n",
    "\n",
    "utils.print_generated_sample_outputs(sample_outputs, tokenizer)\n",
    "# lmao this is worse, TODO "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76293e17429ae2c839469a84a4692f69b1d764ad81a6a044a99c52430bb10e84"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
